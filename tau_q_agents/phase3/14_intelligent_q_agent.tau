# Intelligent Q-Agent: Deflationary Trader with ASCII Communication
#
# This agent demonstrates Q-learning-like behavior through:
# - State-dependent actions (position tracking)
# - Reward-seeking (profit via momentum)
# - Penalty avoidance (no selling at loss)
# - Deflationary burns on profit
#
# INPUTS:
#   i1 = price_up (market signal)
#   i2 = price_down (market signal)
#
# STATE (internal):
#   o5 = position (0=idle, 1=holding)
#   o6 = entry_was_momentum (bought on 2 consecutive ups)
#
# ACTIONS (outputs):
#   o1 = buy_signal
#   o2 = sell_signal  
#   o3 = burn_signal (only on profitable sell)
#   o4 = hold_signal (actively holding)
#
# Q-POLICY (encoded in spec):
# - BUY when: price_up AND prev_price_up AND NOT holding (momentum entry)
# - SELL when: price_down AND holding AND entry_was_momentum (only sell good entries)
# - BURN when: selling (all momentum sells are profitable by definition)
# - HOLD when: holding AND NOT selling

sbf i1 = console
sbf i2 = console
sbf o1 = console
sbf o2 = console
sbf o3 = console
sbf o4 = console
sbf o5 = console
sbf o6 = console

# Buy on momentum (2 consecutive ups, not holding)
# Sell on down when holding momentum entry
# Burn = sell (momentum entries are profitable)
# Hold = holding and not selling
# Position: set on buy, reset on sell
# Entry_was_momentum: set on buy (all buys are momentum)

r o1[t] = i1[t] & i1[t-1] & o5[t-1]' && o2[t] = i2[t] & o5[t-1] & o6[t-1] && o3[t] = o2[t] && o4[t] = o5[t-1] & o2[t]' && o5[t] = o1[t] | (o5[t-1] & o2[t]') && o6[t] = o1[t] | (o6[t-1] & o2[t]')

0
0
0
0
1
0
1
0
0
0
0
1
0
0
q

